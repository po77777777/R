---
title: "Final Project"
author: "Gaeun Lee"
date: "12/4/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(tidyverse)
library(glmnet)
library(FNN)
library(boot)
library(leaps)
```


```{r function, include=FALSE}
#convert NA to No
na.to.no <- function(thevec){
  as.factor(ifelse(is.na(thevec),
                   "No",
                   as.character(thevec)))
}

#Convert Quality to scale 0-5
qual.to.no <- function(thevec){
  thevec <- as.character(thevec)
  for (i in 1:length(thevec)){
    if (is.na(thevec[i])){
      thevec[i] <- NA
    }
    else if (thevec[i]=="Ex"){
      thevec[i] <- "5"
    }
    else if (thevec[i]=="Gd"){
      thevec[i] <- "4"
    }
    else if (thevec[i]=="TA"){
      thevec[i] <- "3"
    }
    else if (thevec[i]=="Fa"){
      thevec[i] <- "2"
    }
    else if (thevec[i]=="Po"){
      thevec[i] <- "1"
    }
    else if (thevec[i]=="No"){
      thevec[i] <- "0"
    }
  }
  return(as.numeric(thevec))
}

#random fill factors
random.fill <- function(vec){
  for (i in 1:length(vec)){
    if (is.na(vec[i])){
      vec[i] <- sample(levels(vec),1)
    }
  }
  return(vec)
}

#random fill number
random.fill.num <- function(vec){
  for (i in 1:length(vec)){
    if (is.na(vec[i])){
      vec[i] <- sample(min(vec,na.rm=TRUE):max(vec,na.rm = TRUE),1)
    }
  }
  return(vec)
}

#subset selection predict
predict.regsubsets <- function (object, newdata , id, ...){ 
  form <- as.formula(object$call[[2]]) # formula of null model 
  mat <- model.matrix(form, newdata) # building an "X" matrix from newdata
  coefi <- coef(object, id = id) # coefficient estimates associated with the object model containing id non-zero variables
  xvars <- names(coefi) # names of the non-zero coefficient estimates
  return(mat[,xvars] %*% coefi) # X[,non-zero variables] %*% Coefficients[non-zero variables]
}

# write csv
csv.function <- function(pred,name="abc.csv"){
pred.knn <- as.data.frame(pred)
pred.knn<-cbind((1:dim(pred.knn)[1]),pred.knn)
colnames(pred.knn)<-c("Id","SalePrice")
pred.knn$Id <- pred.knn$Id + 1460
write.csv(pred.knn,name,row.names = FALSE)
}
```

## Introduction to the data an dproblem

```{r load data}
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

## Explain the raw data

```{r data}
glimpse(train)
colSums(is.na(train))
colSums(is.na(test))
```

## Explain how you clean the data

### train data

```{r train}
#MSSubClass to factor
train$MSSubClass <-as.factor(train$MSSubClass)

#Change NA to No
train$Alley <- na.to.no(thevec=train$Alley)
train$BsmtQual <- na.to.no(thevec=train$BsmtQual)
train$BsmtCond <- na.to.no(thevec=train$BsmtCond)
train$BsmtExposure <- as.factor(ifelse(is.na(train$BsmtExposure),
                                       "NoB",
                                       as.character(train$BsmtExposure)))
train$BsmtFinType1 <- na.to.no(train$BsmtFinType1)
train$BsmtFinType2 <- na.to.no(train$BsmtFinType2)
train$FireplaceQu <- na.to.no(train$FireplaceQu)
train$GarageType <- na.to.no(train$GarageType)
train$GarageFinish <- na.to.no(train$GarageFinish)
train$GarageQual <- na.to.no(train$GarageQual)
train$GarageCond <- na.to.no(train$GarageCond)
train$PoolQC <- na.to.no(train$PoolQC)
train$Fence <- na.to.no(train$Fence)
train$MiscFeature <- na.to.no(train$MiscFeature)

train <- subset(train, select = - GarageYrBlt)
train <- subset(train, select = - LotFrontage)

# Change to 1-5
train$ExterQual <- qual.to.no(train$ExterQual)
train$ExterCond <- qual.to.no(train$ExterCond)
train$BsmtQual <- qual.to.no(train$BsmtQual)
train$BsmtCond <- qual.to.no(train$BsmtCond)
train$HeatingQC <- qual.to.no(train$HeatingQC)
train$KitchenQual <- qual.to.no(train$KitchenQual)
train$FireplaceQu <- qual.to.no(train$FireplaceQu)
train$GarageQual <- qual.to.no(train$GarageQual)
train$GarageCond <- qual.to.no(train$GarageCond)
train$PoolQC <- qual.to.no(train$PoolQC)

#Remove NAs
train <- na.omit(train)

#exclude Id
train <- train[,-1]

#Exclude Utlilities because not in test
train <- subset(train,select = - Utilities)
colSums(is.na(train))
```

### test data

```{r test}
#MSSubClass to factor
test$MSSubClass <-as.factor(test$MSSubClass)

#Change NA to No
test$Alley <- na.to.no(thevec=test$Alley)
test$BsmtQual <- na.to.no(thevec=test$BsmtQual)
test$BsmtCond <- na.to.no(thevec=test$BsmtCond)
test$BsmtExposure <- as.factor(ifelse(is.na(test$BsmtExposure),
                                      "NoB",
                                      as.character(test$BsmtExposure)))
test$BsmtFinType1 <- na.to.no(test$BsmtFinType1)
test$BsmtFinType2 <- na.to.no(test$BsmtFinType2)
test$FireplaceQu <- na.to.no(test$FireplaceQu)
test$GarageType <- na.to.no(test$GarageType)
test$GarageFinish <- na.to.no(test$GarageFinish)
test$GarageQual <- na.to.no(test$GarageQual)
test$GarageCond <- na.to.no(test$GarageCond)
test$PoolQC <- na.to.no(test$PoolQC)
test$Fence <- na.to.no(test$Fence)
test$MiscFeature <- na.to.no(test$MiscFeature)

# Because GarageYrBlt is highly correlated with YearBuilt, I decided to remove GarageYrBlt vector
test <- subset(test, select = - GarageYrBlt)
# So I decided to drop LotFrontage
test <- subset(test, select = - LotFrontage)

# Change to 1-5
test$ExterQual <- qual.to.no(test$ExterQual)
test$ExterCond <- qual.to.no(test$ExterCond)
test$BsmtQual <- qual.to.no(test$BsmtQual)
test$BsmtCond <- qual.to.no(test$BsmtCond)
test$HeatingQC <- qual.to.no(test$HeatingQC)
test$KitchenQual <- qual.to.no(test$KitchenQual)
test$FireplaceQu <- qual.to.no(test$FireplaceQu)
test$GarageQual <- qual.to.no(test$GarageQual)
test$GarageCond <- qual.to.no(test$GarageCond)
test$PoolQC <- qual.to.no(test$PoolQC)

#exclude Id
test <- test[,-1]
#exclude Utilities because test data only have 1 level
test <- subset(test,select = - Utilities)

#random fill
test$MSZoning <- random.fill(test$MSZoning)
test$Exterior1st <- random.fill(test$Exterior1st)
test$Exterior2nd <- random.fill(test$Exterior2nd)
test$MasVnrType <- random.fill(test$MasVnrType)
test$MasVnrArea <- random.fill.num(test$MasVnrArea)
test$BsmtFinSF1 <- random.fill.num(test$BsmtFinSF1)
test$BsmtFinSF2 <- random.fill.num(test$BsmtFinSF2)
test$BsmtUnfSF <- random.fill.num(test$BsmtUnfSF)
test$TotalBsmtSF <- random.fill.num(test$TotalBsmtSF)
test$BsmtFullBath <- random.fill.num(test$BsmtFullBath)
test$BsmtHalfBath <- random.fill.num(test$BsmtHalfBath)
test$KitchenQual <- random.fill.num(test$KitchenQual)
test$Functional <- random.fill(test$Functional)
test$GarageCars <- random.fill.num(test$GarageCars)
test$GarageArea <- random.fill.num(test$GarageArea)
test$SaleType <- random.fill(test$SaleType)

colSums(is.na(test))
```

### Equal levels

```{r levels}
# indicator
test$data <- rep("test",length=nrow(test))
train$data <- rep("train",length=nrow(train))

# join
train.test <- rbind(test,train)

# split
test1 <- subset(train.test,subset=data=="test")
test1 <- test1[,-ncol(test1)]
train1 <- subset(train.test,subset=data=="train")
train1 <- train1[,-ncol(train1)]
```

### model matrix

```{r matrix}
test.X <- model.matrix(SalePrice~.,test1)[,-1]
train.X <- model.matrix(SalePrice~.,train1)[,-1]
test.y <- test1$SalePrice
train.y <- train1$SalePrice
dim(test.X)
dim(train.X)
```

## Statistical learning method:

### Linear regression

```{r lm}
#model
train1.lm <- subset(train1,select=-MSSubClass)
test1.lm <- subset(test1,select=-MSSubClass)
lm.pred <- lm(log(SalePrice)~.,data=train1.lm)
pred.lm <- exp(predict(lm.pred,test1.lm))
par(mfrow=c(2,2))
plot(lm.pred)

pred.lm <- as.vector(pred.lm)

csv.function(pred.lm,name="lm.csv")
```

### Subset selection methods

#### best subset

```{r best subset}
best.subset <- regsubsets(log(SalePrice)~.,data=train1,nvmax=3,really.big=TRUE)
best.summary <- summary(best.subset)

par(mfrow=c(2,2))

# RSS plot
plot(best.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")

# adjr2 plot
adjr2.best <- which.max(best.summary$adjr2)
plot(best.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
points(adjr2.best,best.summary$adjr2[adjr2.best], col = "red", cex = 2, pch = 20)

pred.best <- exp(predict.regsubsets(best.subset,test1,id=adjr2.best))
csv.function(pred.best,name="best.csv")

# Cp plot
cp.best <- which.min(best.summary$cp)
plot(best.summary$cp ,xlab="Number of Variables ", ylab="Cp", type="l")
points(cp.best,best.summary$cp[cp.best], col = "red", cex = 2, pch = 20)

# BIC plot
bic.best <- which.min(best.summary$bic)
plot(best.summary$bic ,xlab="Number of Variables ", ylab="BIC", type="l")
points(bic.best,best.summary$bic[bic.best], col = "red", cex = 2, pch = 20)

```

#### forward subset

```{r forward}
forward.subset <- regsubsets(log(SalePrice)~.,data=train1,nvmax=200,method="forward",really.big=TRUE)
for.summary <- summary(forward.subset)

par(mfrow=c(2,2))

# RSS plot
plot(for.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")

# adjr2 plot
adjr2.for <- which.max(for.summary$adjr2)
plot(for.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
points(adjr2.for,for.summary$adjr2[adjr2.for], col = "red", cex = 2, pch = 20)

# Cp plot
cp.for <- which.min(for.summary$cp)
plot(for.summary$cp ,xlab="Number of Variables ", ylab="Cp", type="l")
points(cp.for,for.summary$cp[cp.for], col = "red", cex = 2, pch = 20)

# BIC plot
bic.for <- which.min(for.summary$bic)
plot(for.summary$bic ,xlab="Number of Variables ", ylab="BIC", type="l")
points(bic.for,for.summary$bic[bic.for], col = "red", cex = 2, pch = 20)

pred.forward <- exp(predict.regsubsets(forward.subset,test1,id=bic.for))
csv.function(pred.forward,name="forward.csv")

#No of variables
c(adjr2.for,cp.for,bic.for)
```

#### backward subset

```{r backward}
back.subset <- regsubsets(log(SalePrice)~.,data=train1,nvmax=200,method="backward",really.big=TRUE)
back.summary <- summary(back.subset)

par(mfrow=c(2,2))

# RSS plot
plot(back.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")

# adjr2 plot
adjr2.back <- which.max(back.summary$adjr2)
plot(back.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
points(adjr2.back,back.summary$adjr2[adjr2.back], col = "red", cex = 2, pch = 20)

# Cp plot
cp.back <- which.min(back.summary$cp)
plot(back.summary$cp ,xlab="Number of Variables ", ylab="Cp", type="l")
points(cp.back,back.summary$cp[cp.back], col = "red", cex = 2, pch = 20)

# BIC plot
bic.back <- which.min(back.summary$bic)
plot(back.summary$bic ,xlab="Number of Variables ", ylab="BIC", type="l")
points(bic.back,back.summary$bic[bic.back], col = "red", cex = 2, pch = 20)

pred.back <- exp(predict.regsubsets(back.subset,test1,id=bic.back))
csv.function(pred.back,name="back.csv")

#No of variables
c(adjr2.back,cp.back,bic.back)
```

### Shrinkage methods

#### Ridge

```{r ridge}
grid <- 10^seq(10,-2,length=100)

###model
ridge.mod <- glmnet(train.X,log(train.y),alpha=0,lambda=grid)

###cross-validation
cv.ridge <- cv.glmnet(train.X,log(train.y),alpha=0,nfolds = 10)
plot(cv.ridge)
bestlam.ridge <- cv.ridge$lambda.min
coef.ridge <- coef(ridge.mod,s=bestlam.ridge)

### extract coef
length(colnames(train.X)[which(coef(cv.ridge, s = bestlam.ridge) != 0)])

pred.ridge <- exp(predict(ridge.mod,s=bestlam.ridge,newx=test.X,type="response"))
csv.function(pred.ridge,name="ridge.csv")
```

#### lasso

```{r lasso}
###model
lasso.mod <- glmnet(train.X,log(train.y),alpha=1,lambda=grid)

###cross-validation
cv.lasso <- cv.glmnet(train.X,log(train.y),alpha=1)
plot(cv.lasso)
bestlam.lasso <- cv.lasso$lambda.min
coef.lasso <- coef(lasso.mod,s=bestlam.lasso)

### no of variables
length(colnames(train.X)[which(coef(cv.lasso, s = bestlam.lasso) != 0)])

pred.lasso <- exp(predict(lasso.mod,s=bestlam.lasso,newx=test.X,type="response"))
csv.function(pred.lasso,name="lasso.csv")
```

### Generalized additive model

### Regression tree

Tree model

```{r tree}
library(tree)

tree.fit <- tree(SalePrice~.,data=train1)
summary(tree.fit)
tree.fit

par(mfrow=c(1,1))
plot(tree.fit) 
text(tree.fit, pretty = 2)

pred.tree <- predict(tree.fit,newdata=test1)
csv.function(pred.tree,name="tree.csv")
```

cross-validate tree

```{r}
tree.cv <- cv.tree(tree.fit,K=10)

best.size <- tree.cv$size[which.min(tree.cv$dev)]
best.size
```

prune tree

```{r}
pruned <- prune.tree(tree.fit,
                           best=best.size)
par(mfrow=c(1,1))
plot(pruned)
text(pruned,pretty=0)

pred.pruned <- predict(pruned,test1)
csv.function(pred.pruned,name="pruned.csv")
```

### Bagging

```{r bag}
library(randomForest)

bag <- randomForest(log(SalePrice)~.,
                           data=train1,
                           mtry=76,
                           importance=TRUE,
                           ntree=1000)

important <- importance(bag)[importance(bag)[,1]>22,1]

barplot(importance(bag)[names(important),1], col="blue", 
        xlab="Predictors",ylab="Importance",main="Bagging")

pred.bag <- exp(predict(bag,newdata=test1))
csv.function(pred.bag,name="bag.csv")
```

### Random Forest

```{r forest}
forest <- randomForest(log(SalePrice)~.,
                           data=train1,
                           mtry=9,
                           importance=TRUE,
                           ntree=1000)

important <- importance(forest)[importance(forest)[,1]>22,1]

barplot(importance(forest)[names(important),1], col="blue", 
        xlab="Predictors",ylab="Importance",main="Random Forest")

pred.forest <- exp(predict(forest,test1))
csv.function(pred.forest,name="forest.csv")
```

### Boosting

```{r}
library(gbm)
gbm.cv <- gbm(log(SalePrice)~., data=train1,
                     distribution = "gaussian",
                     shrinkage = 0.01,
                     n.tree=1000, 
                     interaction.depth = 4,
                     cv.folds=10)


summary(gbm.cv)

which.min(gbm.cv$cv.error)

pred.boost <- exp(predict(gbm.cv,test1,n.trees = which.min(gbm.cv$cv.error)))
csv.function(pred.boost,name="boost.csv")
```

### KNN

```{r}

fold.index <- cut(sample(1:nrow(train.X)),
                  breaks=10, labels=FALSE)

K <- c(1,5,10,15,20,25,30)
mse.df <- rep(NA,length=7)
mse.k <- rep(NA,length=10)
n <- 1
for (k in K){
  for (i in 1:10){
    cvknn <- knn.reg(train.X[fold.index!=i,],
                    train.X[fold.index==i,],
                    train.y[fold.index!=i],
                    k=k)
    pred <- cvknn$pred
    mse <- mean((pred-train.y[fold.index==i])^2)
    mse.k[i] <- mse
  }
  mse.df[n] <- mean(mse.k)
  n <- n+1
}
mse.df <- data.frame(mse.df)
row.names(mse.df) <- c(1,5,10,15,20,25,30)
which.min(mse.df$mse.df) # K=10 is the best

knn.fit <- knn.reg(train.X,
                  test.X,
                  train.y,k=10)

pred.knn <- knn.fit$pred


csv.function(pred.knn,"knn.csv")
```

TRUE TEST ERROR : 0.24294

### Estimated Test Error

#### KNN

```{r}

fold.index <- cut(sample(1:nrow(train.X)),
                  breaks=10, labels=FALSE)

K <- c(1,5,10,15,20,25,30)
mse.df <- rep(NA,length=7)
mse.k <- rep(NA,length=10)
n <- 1
for (k in K){
  for (i in 1:10){
    cvknn <- knn.reg(train.X[fold.index!=i,],
                    train.X[fold.index==i,],
                    log(train.y[fold.index!=i]),
                    k=k)
    pred <- cvknn$pred
    mse <- mean((pred-log(train.y[fold.index==i]))^2)
    mse.k[i] <- mse
  }
  mse.df[n] <- mean(mse.k)
  n <- n+1
}
mse.df <- data.frame(mse.df)
row.names(mse.df) <- c(1,5,10,15,20,25,30)
min(mse.df$mse.df) # K=10
```

The least mse for tuning paramater K cross-validation is K=10 and K=5. This agrees with true test error

#### Linear model
```{r}
# train1.lm <- subset(train1,select=-c(MSSubClass,
#                                      BldgType,
#                                      Exterior2nd,
#                                      TotalBsmtSF,
#                                      GrLivArea,
#                                      GarageFinish))
# 
# lm.pred <- lm(SalePrice~.,data=train1.lm[fold.index!=i,])
# pred.lm <- data.frame(predict(lm.pred,
#                               train1.lm[fold.index==i,]))
# 
# error.vec <- rep(NA,length=10)
# 
# for (i in 1:10){
#   glm.fit <- lm(SalePrice~.,
#                  data=train1[fold.index!=i,])
#   predict(glm.fit,train1[fold.index==i,])
# }
#glm.fit <- glm(SalePrice~.,data=train1)
#cv.error <- cv.glm(train,glm.fit,K=10)$delta[1]
```

#### Subset selection

##### Best Subset

```{r}
# fold.index <- cut(sample(1:nrow(train1)), breaks=10, labels=FALSE)
# 
# for (i in 1:adjr2.best){
#   cat("i=", i,"\n")
#   error <- rep(0,10)
#   for(k in 1:10){
#     train1.train <- train1[fold.index!=k,]
#     train1.test <- train1[fold.index==k,]
#     true.y <- train1.test[,"SalePrice"]
#     best.fit <- regsubsets(SalePrice~.,data=train1.train,
#                            nvmax=3,really.big = TRUE)
#     pred <- predict(best.fit,train1.test,id=i)
#     error[k] <- mean((pred-true.y)^2)
#   }
#   print(mean(error))
#   cv.error.best.fit[i] <- mean(error)
# }

```

It takes too long to do cross-validation

##### Forward Subset

```{r,results="hide", echo=TRUE}
fold.index <- cut(sample(1:nrow(train1)), breaks=10, labels=FALSE)

cv.error.best.fit <- rep(0,50)

for (i in 1:50){
  cat("i=", i,"\n")
  error <- rep(0,10)
  for(k in 1:10){
    train1.train <- train1[fold.index!=k,]
    train1.test <- train1[fold.index==k,]
    true.y <- train1.test[,"SalePrice"]
    best.fit <- regsubsets(log(SalePrice)~.,data=train1.train,
                           nvmax=50,really.big = TRUE,
                           method="forward")
    pred <- predict(best.fit,train1.test,id=i)
    error[k] <- mean((pred-log(true.y))^2)
  }
  #print(mean(error))
  cv.error.best.fit[i] <- mean(error)
}
```

```{r}
c(which.min(cv.error.best.fit),cv.error.best.fit[which.min(cv.error.best.fit)])
```

lowest CV estimated test error for forward is with 50 predictors (nvmax=50)

##### Backward Subset

```{r, results="hide", echo=TRUE}
fold.index <- cut(sample(1:nrow(train1)), breaks=10, labels=FALSE)

cv.error.best.fit <- rep(0,50)

for (i in 1:50){
  cat("i=", i,"\n")
  error <- rep(0,10)
  for(k in 1:10){
    train1.train <- train1[fold.index!=k,]
    train1.test <- train1[fold.index==k,]
    true.y <- train1.test[,"SalePrice"]
    best.fit <- regsubsets(log(SalePrice)~.,data=train1.train,
                           nvmax=50,really.big = TRUE,
                           method="backward")
    pred <- predict(best.fit,train1.test,id=i)
    error[k] <- mean((pred-log(true.y))^2)
  }
  print(mean(error))
  cv.error.best.fit[i] <- mean(error)
}
```

```{r}
c(which.min(cv.error.best.fit),cv.error.best.fit[which.min(cv.error.best.fit)])
```

lowest CV estimated test error for backward is with 44 predictors (nvmax=50)

#### Shrinkage Method

##### Ridge Regression

```{r}
###model
ridge.mod <- glmnet(train.X,log(train.y),alpha=0,lambda=grid)

###cross-validation
cv.ridge <- cv.glmnet(train.X,log(train.y),alpha=0,nfolds = 10)
c(which.min(cv.ridge$cvm),cv.ridge$cvm[which.min(cv.ridge$cvm)])

#train.y before transformation
```

lowest mse is when there are 82 predictors

##### Lasso Regression

```{r}
###model
lasso.mod <- glmnet(train.X,log(train.y),alpha=1,lambda=grid)

###cross-validation
cv.lasso <- cv.glmnet(train.X,log(train.y),alpha=1)
c(which.min(cv.lasso$cvm),cv.lasso$cvm[which.min(cv.lasso$cvm)])

# Use train.y before log transformation

```

lowest mse is when there are 41 predictors

#### Estimated Test Error

```{r}
Model <- c("knn","Forward Subset(45 predictors)",
          "Backward Subset(50 predictors)","Ridge Regression",
          "Lasso Regression(49 predictors)")

est.error <- c(0.05043,0.02973,0.03348,0.02050,0.02125)

est.df <- data.frame(Model,est.error)
est.df
```

#### true test error

```{r}
Model <- c("knn","linear model","Best Subset(nvmax=3)","Forward Subset(adjr2,140 predictors)",
          "Backward Subset(adjr2,143 predictors)","Ridge Regression(lambda=0.1585827)",
          "Lasso Regression(lambda=0.004115261)")

true.error <- c(0.24094,0.13704,0.27518,0.16819,0.16689,0.13225,0.13156)

true.df <- data.frame(Model,true.error)
true.df
```
